<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="AutoImportSettings">
    <option name="autoReloadType" value="SELECTIVE" />
  </component>
  <component name="ChangeListManager">
    <list default="true" id="c978e7d7-4b1e-45bc-a695-e10813607882" name="Changes" comment="#!/usr/bin/env bash&#10;ps -ef|grep -E 'RayWorkerWrapper|vllm.entrypoints.openai.api_server'|awk '{print $2}'| xargs kill -9">
      <change afterPath="$PROJECT_DIR$/run_llama.sh" afterDir="false" />
      <change afterPath="$PROJECT_DIR$/run_mistral.sh" afterDir="false" />
      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
      <change beforePath="$PROJECT_DIR$/README.md" beforeDir="false" afterPath="$PROJECT_DIR$/README.md" afterDir="false" />
      <change beforePath="$PROJECT_DIR$/run_internlm.sh" beforeDir="false" afterPath="$PROJECT_DIR$/run_internlm.sh" afterDir="false" />
    </list>
    <option name="SHOW_DIALOG" value="false" />
    <option name="HIGHLIGHT_CONFLICTS" value="true" />
    <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
    <option name="LAST_RESOLUTION" value="IGNORE" />
  </component>
  <component name="Git.Settings">
    <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
  </component>
  <component name="GitHubPullRequestSearchHistory">{
  &quot;lastFilter&quot;: {
    &quot;state&quot;: &quot;OPEN&quot;,
    &quot;assignee&quot;: &quot;Jason-Chen-2017&quot;
  }
}</component>
  <component name="GithubPullRequestsUISettings">{
  &quot;selectedUrlAndAccountId&quot;: {
    &quot;url&quot;: &quot;https://github.com/LLMAppArchitect/chatglm4-vllm&quot;,
    &quot;accountId&quot;: &quot;6b7f8026-a38d-4100-b0ac-0dff316486dc&quot;
  }
}</component>
  <component name="ProjectColorInfo">{
  &quot;customColor&quot;: &quot;&quot;,
  &quot;associatedIndex&quot;: 8
}</component>
  <component name="ProjectId" id="2gxQe1xbmLzZ2SzfWf3D1jiHnry" />
  <component name="ProjectViewState">
    <option name="autoscrollFromSource" value="true" />
    <option name="hideEmptyMiddlePackages" value="true" />
    <option name="showLibraryContents" value="true" />
  </component>
  <component name="PropertiesComponent">{
  &quot;keyToString&quot;: {
    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
    &quot;git-widget-placeholder&quot;: &quot;master&quot;,
    &quot;last_opened_file_path&quot;: &quot;/home/me/PycharmProjects/chatglm4-vllm&quot;,
    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,
    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,
    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,
    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,
    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;
  }
}</component>
  <component name="RecentsManager">
    <key name="CopyFile.RECENT_KEYS">
      <recent name="$PROJECT_DIR$" />
    </key>
  </component>
  <component name="SharedIndexes">
    <attachedChunks>
      <set>
        <option value="bundled-js-predefined-1d06a55b98c1-0b3e54e931b4-JavaScript-PY-241.18034.82" />
        <option value="bundled-python-sdk-975db3bf15a3-2767605e8bc2-com.jetbrains.pycharm.pro.sharedIndexes.bundled-PY-241.18034.82" />
      </set>
    </attachedChunks>
  </component>
  <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="application-level" UseSingleDictionary="true" transferred="true" />
  <component name="TaskManager">
    <task active="true" id="Default" summary="Default task">
      <changelist id="c978e7d7-4b1e-45bc-a695-e10813607882" name="Changes" comment="" />
      <created>1716636841006</created>
      <option name="number" value="Default" />
      <option name="presentableId" value="Default" />
      <updated>1716636841006</updated>
      <workItem from="1716636845373" duration="624000" />
      <workItem from="1716641954410" duration="9000" />
      <workItem from="1717416802248" duration="957000" />
      <workItem from="1717585365486" duration="1616000" />
      <workItem from="1717587882771" duration="2189000" />
      <workItem from="1718624144668" duration="1065000" />
      <workItem from="1719035581549" duration="1301000" />
      <workItem from="1719230232515" duration="280000" />
      <workItem from="1719248572374" duration="106000" />
      <workItem from="1719727380745" duration="830000" />
      <workItem from="1719730476733" duration="27000" />
      <workItem from="1719731596276" duration="1940000" />
      <workItem from="1719750367176" duration="1299000" />
      <workItem from="1719946933369" duration="503000" />
      <workItem from="1720363929640" duration="7180000" />
      <workItem from="1721664516388" duration="653000" />
      <workItem from="1721666765127" duration="283000" />
      <workItem from="1722092608104" duration="4003000" />
    </task>
    <task id="LOCAL-00001" summary="    prompt = [&#10;        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: example_prompt},&#10;        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: example_blog},&#10;        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input_data.prompt},&#10;    ]">
      <option name="closed" value="true" />
      <created>1719036411962</created>
      <option name="number" value="00001" />
      <option name="presentableId" value="LOCAL-00001" />
      <option name="project" value="LOCAL" />
      <updated>1719036411962</updated>
    </task>
    <task id="LOCAL-00002" summary=".idea">
      <option name="closed" value="true" />
      <created>1719036439765</created>
      <option name="number" value="00002" />
      <option name="presentableId" value="LOCAL-00002" />
      <option name="project" value="LOCAL" />
      <updated>1719036439765</updated>
    </task>
    <task id="LOCAL-00003" summary="https://huggingface.co/THUDM/glm-4-9b-chat">
      <option name="closed" value="true" />
      <created>1719036754710</created>
      <option name="number" value="00003" />
      <option name="presentableId" value="LOCAL-00003" />
      <option name="project" value="LOCAL" />
      <updated>1719036754710</updated>
    </task>
    <task id="LOCAL-00004" summary="example_blog2">
      <option name="closed" value="true" />
      <created>1719248665042</created>
      <option name="number" value="00004" />
      <option name="presentableId" value="LOCAL-00004" />
      <option name="project" value="LOCAL" />
      <updated>1719248665042</updated>
    </task>
    <task id="LOCAL-00005" summary="add: &#10;    print(seg)&#10;    t = datetime.now().strftime('%Y-%m-%d %H:%M:%S')&#10;    end = int(time.time())&#10;    print(f'结束时间:{t}')&#10;    print(f'总字数:{len(text)}')&#10;    print(f'耗时：{end - start} 秒)')">
      <option name="closed" value="true" />
      <created>1719947098039</created>
      <option name="number" value="00005" />
      <option name="presentableId" value="LOCAL-00005" />
      <option name="project" value="LOCAL" />
      <updated>1719947098039</updated>
    </task>
    <task id="LOCAL-00006" summary="RUN LLM internlm2_5-7b-chat&#10;&#10;#!/usr/bin/env bash&#10;RAY_memory_monitor_refresh_ms=0 RAY_memory_usage_threshold=0.8 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python -m vllm.entrypoints.openai.api_server --model internlm/internlm2_5-7b-chat --served-model-name internlm2_5-7b-chat --trust-remote-code --port 8082 --device cuda --tensor-parallel-size 2 --gpu-memory-utilization 0.95 --swap-space 1">
      <option name="closed" value="true" />
      <created>1720374596623</created>
      <option name="number" value="00006" />
      <option name="presentableId" value="LOCAL-00006" />
      <option name="project" value="LOCAL" />
      <updated>1720374596623</updated>
    </task>
    <task id="LOCAL-00007" summary="#!/usr/bin/env bash&#10;ps -ef|grep -E 'RayWorkerWrapper|vllm.entrypoints.openai.api_server'|awk '{print $2}'| xargs kill -9">
      <option name="closed" value="true" />
      <created>1721667039509</created>
      <option name="number" value="00007" />
      <option name="presentableId" value="LOCAL-00007" />
      <option name="project" value="LOCAL" />
      <updated>1721667039509</updated>
    </task>
    <option name="localTasksCounter" value="8" />
    <servers />
  </component>
  <component name="TypeScriptGeneratedFilesManager">
    <option name="version" value="3" />
  </component>
  <component name="VcsManagerConfiguration">
    <MESSAGE value="    prompt = [&#10;        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: example_prompt},&#10;        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: example_blog},&#10;        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input_data.prompt},&#10;    ]" />
    <MESSAGE value=".idea" />
    <MESSAGE value="https://huggingface.co/THUDM/glm-4-9b-chat" />
    <MESSAGE value="example_blog2" />
    <MESSAGE value="add: &#10;    print(seg)&#10;    t = datetime.now().strftime('%Y-%m-%d %H:%M:%S')&#10;    end = int(time.time())&#10;    print(f'结束时间:{t}')&#10;    print(f'总字数:{len(text)}')&#10;    print(f'耗时：{end - start} 秒)')" />
    <MESSAGE value="RUN LLM internlm2_5-7b-chat&#10;&#10;#!/usr/bin/env bash&#10;RAY_memory_monitor_refresh_ms=0 RAY_memory_usage_threshold=0.8 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python -m vllm.entrypoints.openai.api_server --model internlm/internlm2_5-7b-chat --served-model-name internlm2_5-7b-chat --trust-remote-code --port 8082 --device cuda --tensor-parallel-size 2 --gpu-memory-utilization 0.95 --swap-space 1" />
    <MESSAGE value="#!/usr/bin/env bash&#10;ps -ef|grep -E 'RayWorkerWrapper|vllm.entrypoints.openai.api_server'|awk '{print $2}'| xargs kill -9" />
    <option name="LAST_COMMIT_MESSAGE" value="#!/usr/bin/env bash&#10;ps -ef|grep -E 'RayWorkerWrapper|vllm.entrypoints.openai.api_server'|awk '{print $2}'| xargs kill -9" />
  </component>
</project>